🔹 How to Explain in Interview

1. Project Overview
“My project is an AI-Based Autonomous Navigation System. The goal is to make a robot or self-driving car understand its surroundings using a camera and sensors, and then take driving decisions like moving left, right, straight, or stop.”

2. Input & Output (Simple Explanation)

Input: A road video (input.mp4) which acts like the robot’s camera feed.

Processing:
Lane Detection → detects the road lanes using OpenCV (edge detection + Hough transform).
Object Detection → detects vehicles, pedestrians, obstacles using YOLO.
Decision Making → calculates steering angle & decides motor speeds.

Output: A processed video (output.mp4) with:
Lane lines drawn
Detected objects boxed & labeled
Steering angle shown
Motor speed signals (Left/Right wheel %) displayed

3. Real-Life Use Cases

Self-Driving Cars → Tesla, Waymo use similar techniques for lane keeping & obstacle avoidance.

Delivery Robots → Companies like Amazon & Starship use AI navigation to deliver packages.

Drones / Autonomous Vehicles → Used for surveillance, defense, agriculture robots.

🔹 One-Liner to Impress

“This project is basically a mini self-driving car brain—it takes video as input, uses AI + computer vision to understand lanes and obstacles, and outputs driving decisions just like an autonomous vehicle does in real life.”

"A popup window will show the AI decisions frame by frame (like a live robot camera).
At the same time, output.mp4 will be saved for later.
Press q anytime to stop early.""